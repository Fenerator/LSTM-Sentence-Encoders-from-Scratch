{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get BERT Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepare input\n",
    "# text = \"Replace me by any text you'd like BERT to be fine-tuned on\"\n",
    "# encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# # forward pass\n",
    "# output = model(**encoded_input, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_hidden_state = output.hidden_states[-1] # shape: batch_size, sequence_length, hidden_size\n",
    "# h_s = output.hidden_states # tuple of length 13\n",
    "\n",
    "# # Extract the CLS embedding\n",
    "# cls_embedding = last_hidden_state[:, 0, :]\n",
    "\n",
    "# last_hidden_state.shape, cls_embedding.shape, len(h_s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Sentences from Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "import pandas as pd\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 0\n",
      "Errors: 0\n",
      "Errors: 0\n"
     ]
    }
   ],
   "source": [
    "def extract_words_from_tree(tree):\n",
    "    words = []\n",
    "    if isinstance(tree, str):  # Base case: leaf node (word)\n",
    "        return [tree]\n",
    "    \n",
    "    elif isinstance(tree, Tree):\n",
    "        for subtree in tree:\n",
    "            words.extend(extract_words_from_tree(subtree))\n",
    "            \n",
    "    #sentence = \" \".join(words)\n",
    "    return words\n",
    "\n",
    "\n",
    "def extract_sent_list_from_tree_file(PATH):\n",
    "    \n",
    "    with open(PATH, \"r\", encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    sentences = []\n",
    "    counter = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        try:\n",
    "            tree = Tree.fromstring(line)\n",
    "        except ValueError:\n",
    "            try: # remove last ')'\n",
    "                tree = Tree.fromstring(line[:-1])\n",
    "                \n",
    "            except ValueError:\n",
    "                counter += 1\n",
    "                print(f\"=== ValueError: line {i} \\n {line} ===\")\n",
    "                continue\n",
    "        words = extract_words_from_tree(tree)\n",
    "        sentences.append(words)\n",
    "        \n",
    "    print(f'Errors: {counter}')\n",
    "    return sentences\n",
    "    \n",
    "sentences_FR = extract_sent_list_from_tree_file('/project/gpuuva021/shared/FMRI-Data/annotation/EN/lppEN_tree.csv')\n",
    "sentences_CN = extract_sent_list_from_tree_file('/project/gpuuva021/shared/FMRI-Data/annotation/CN/lppCN_tree.csv')\n",
    "sentences_FR = extract_sent_list_from_tree_file('/project/gpuuva021/shared/FMRI-Data/annotation/FR/lppFR_tree.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['il',\n",
       " 'se',\n",
       " 'trouvait',\n",
       " 'dans',\n",
       " 'la',\n",
       " 'région',\n",
       " 'des',\n",
       " 'astéroïdes',\n",
       " '325',\n",
       " '326',\n",
       " '327',\n",
       " '328',\n",
       " '329',\n",
       " 'et',\n",
       " '330']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TREE TEST\n",
    "line =  '(S (VN (CLS il) (CLR se) (V trouvait)) (PP (P dans) (NP (DET la) (NC région) (PP (P des) (NP (MWN (N astéroïdes) (ADJ 325 326 327 328)))))) (NP (DET 329) (MWN (DET et) (DET 330))))'\n",
    "tree = Tree.fromstring(line)\n",
    "words = extract_words_from_tree(tree)\n",
    "words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Sentence infos from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"/project/gpuuva021/shared/FMRI-Data\")\n",
    "language = \"FR\"\n",
    "\n",
    "# /project/gpuuva021/shared/FMRI-Data/annotation/EN/lppEN_word_information.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "      <th>logfreq</th>\n",
       "      <th>pos</th>\n",
       "      <th>section</th>\n",
       "      <th>top_down</th>\n",
       "      <th>bottom_up</th>\n",
       "      <th>left_corner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>lorsque</td>\n",
       "      <td>lorsque</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>3.420000</td>\n",
       "      <td>1.693375</td>\n",
       "      <td>CS</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>j'</td>\n",
       "      <td>j'</td>\n",
       "      <td>3.420000</td>\n",
       "      <td>3.440000</td>\n",
       "      <td>-5.958607</td>\n",
       "      <td>CLS</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>avais</td>\n",
       "      <td>avais</td>\n",
       "      <td>3.530000</td>\n",
       "      <td>3.760000</td>\n",
       "      <td>2.895069</td>\n",
       "      <td>V</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>six</td>\n",
       "      <td>six</td>\n",
       "      <td>3.930000</td>\n",
       "      <td>4.180000</td>\n",
       "      <td>2.069557</td>\n",
       "      <td>DET</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ans</td>\n",
       "      <td>ans</td>\n",
       "      <td>4.180000</td>\n",
       "      <td>4.360000</td>\n",
       "      <td>2.856227</td>\n",
       "      <td>NC</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15386</th>\n",
       "      <td>15386</td>\n",
       "      <td>ça</td>\n",
       "      <td>ça</td>\n",
       "      <td>661.400000</td>\n",
       "      <td>661.633556</td>\n",
       "      <td>3.952882</td>\n",
       "      <td>PRO</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15387</th>\n",
       "      <td>15387</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>661.633556</td>\n",
       "      <td>661.780000</td>\n",
       "      <td>4.076663</td>\n",
       "      <td>V</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15388</th>\n",
       "      <td>15388</td>\n",
       "      <td>tellement</td>\n",
       "      <td>tellement</td>\n",
       "      <td>661.880000</td>\n",
       "      <td>662.370000</td>\n",
       "      <td>2.264109</td>\n",
       "      <td>ADV</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15389</th>\n",
       "      <td>15389</td>\n",
       "      <td>d'</td>\n",
       "      <td>d'</td>\n",
       "      <td>662.370000</td>\n",
       "      <td>662.420000</td>\n",
       "      <td>3.858826</td>\n",
       "      <td>P</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15390</th>\n",
       "      <td>15390</td>\n",
       "      <td>importance</td>\n",
       "      <td>importance</td>\n",
       "      <td>662.420000</td>\n",
       "      <td>663.160000</td>\n",
       "      <td>1.758306</td>\n",
       "      <td>NC</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15391 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0        word       lemma       onset      offset   logfreq   \n",
       "0               0     lorsque     lorsque    3.050000    3.420000  1.693375  \\\n",
       "1               1          j'          j'    3.420000    3.440000 -5.958607   \n",
       "2               2       avais       avais    3.530000    3.760000  2.895069   \n",
       "3               3         six         six    3.930000    4.180000  2.069557   \n",
       "4               4         ans         ans    4.180000    4.360000  2.856227   \n",
       "...           ...         ...         ...         ...         ...       ...   \n",
       "15386       15386          ça          ça  661.400000  661.633556  3.952882   \n",
       "15387       15387           a           a  661.633556  661.780000  4.076663   \n",
       "15388       15388   tellement   tellement  661.880000  662.370000  2.264109   \n",
       "15389       15389          d'          d'  662.370000  662.420000  3.858826   \n",
       "15390       15390  importance  importance  662.420000  663.160000  1.758306   \n",
       "\n",
       "       pos  section top_down bottom_up left_corner  \n",
       "0       CS        1        3         1           2  \n",
       "1      CLS        1        2         1           2  \n",
       "2        V        1        1         3           2  \n",
       "3      DET        1        2         1           2  \n",
       "4       NC        1        1         2           2  \n",
       "...    ...      ...      ...       ...         ...  \n",
       "15386  PRO        9        2         2           2  \n",
       "15387    V        9        2         2           2  \n",
       "15388  ADV        9        2         1           2  \n",
       "15389    P        9        1         1           1  \n",
       "15390   NC        9        1         6           2  \n",
       "\n",
       "[15391 rows x 11 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "            PATH / f\"annotation/{language}/lpp{language}_word_information.csv\"\n",
    "        )\n",
    "\n",
    "df_list = df['word'].tolist()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lorsque',\n",
       " \"j'\",\n",
       " 'avais',\n",
       " 'six',\n",
       " 'ans',\n",
       " \"j'\",\n",
       " 'ai',\n",
       " 'vu',\n",
       " 'une',\n",
       " 'fois',\n",
       " 'une',\n",
       " 'magnifique',\n",
       " 'image',\n",
       " 'dans',\n",
       " 'un',\n",
       " 'livre',\n",
       " 'sur',\n",
       " 'la',\n",
       " 'forêt',\n",
       " 'vierge',\n",
       " 'qui',\n",
       " \"s'\",\n",
       " 'appelait',\n",
       " 'histoires',\n",
       " 'vécues']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_FR[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['le',\n",
       " 'mouton',\n",
       " 'oui',\n",
       " 'ou',\n",
       " 'non',\n",
       " 'a',\n",
       " 't',\n",
       " 'il',\n",
       " 'mangé',\n",
       " 'la',\n",
       " 'fleur#',\n",
       " 'et',\n",
       " 'vous',\n",
       " 'verrez',\n",
       " 'comme',\n",
       " 'tout',\n",
       " 'change#',\n",
       " 'et',\n",
       " 'aucune',\n",
       " 'grande',\n",
       " 'personne',\n",
       " 'ne',\n",
       " 'comprendra',\n",
       " 'jamais',\n",
       " 'que',\n",
       " 'ça',\n",
       " 'a',\n",
       " 'tellement',\n",
       " \"d'\",\n",
       " 'importance#']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after the last word of each sentence, add # to indicate the end of sentence\n",
    "for i, sent in enumerate(sentences_FR):\n",
    "    sentences_FR[i][-1] = sent[-1] + \"#\" # replace the last word with the word + #\n",
    "\n",
    "# flatten the list of lists of words into a list of words\n",
    "words = [item for sublist in sentences_FR for item in sublist]\n",
    "\n",
    "words[-30:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15392, 15391)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words), len(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_23554/3693703772.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['word'] = words.append(\" \")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>3.05</td>\n",
       "      <td>3.42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>3.93</td>\n",
       "      <td>4.18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>4.18</td>\n",
       "      <td>4.36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word  onset  offset  section\n",
       "0  None   3.05    3.42        1\n",
       "1  None   3.42    3.44        1\n",
       "2  None   3.53    3.76        1\n",
       "3  None   3.93    4.18        1\n",
       "4  None   4.18    4.36        1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# integrate back into df\n",
    "df['word'] = words.append(\" \")\n",
    "\n",
    "# keep only relevant columns\n",
    "df = df[['word', 'onset', 'offset', 'section']]\n",
    "\n",
    "\n",
    "# get the number of unique sections\n",
    "possible_sections = df['section'].unique()\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>once</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.728</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.919</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i</td>\n",
       "      <td>0.919</td>\n",
       "      <td>1.025</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>was</td>\n",
       "      <td>1.025</td>\n",
       "      <td>1.158</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>six</td>\n",
       "      <td>1.158</td>\n",
       "      <td>1.464</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word  onset  offset  section\n",
       "0  once  0.113   0.728        1\n",
       "1  when  0.728   0.919        1\n",
       "2     i  0.919   1.025        1\n",
       "3   was  1.025   1.158        1\n",
       "4   six  1.158   1.464        1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get df by section\n",
    "df1 = df[df['section'] == possible_sections[0]]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'once when i was six years old i saw a magnificent picture in a book about the primeval forest called real life stories.',\n",
       "  'onset': 0.113,\n",
       "  'offset': 8.339,\n",
       "  'section': 1},\n",
       " {'sentence': 'it showed a boa constrictor swallowing a wild animal.',\n",
       "  'onset': 9.247,\n",
       "  'offset': 12.416,\n",
       "  'section': 1},\n",
       " {'sentence': 'here is a copy of the drawing.',\n",
       "  'onset': 13.144,\n",
       "  'offset': 14.464,\n",
       "  'section': 1},\n",
       " {'sentence': 'it said in the book boa constrictors swallow their prey whole without chewing then they are not able to move and they sleep for the six months it takes for digestion.',\n",
       "  'onset': 15.86,\n",
       "  'offset': 26.339,\n",
       "  'section': 1},\n",
       " {'sentence': 'so i thought a lot about the adventures of the jungle and in turn i managed with a colored pencil to make my first drawing my drawing number one.',\n",
       "  'onset': 27.292,\n",
       "  'offset': 36.647,\n",
       "  'section': 1},\n",
       " {'sentence': 'it looked like this.',\n",
       "  'onset': 37.186,\n",
       "  'offset': 38.326,\n",
       "  'section': 1},\n",
       " {'sentence': 'i showed my masterpiece to the grownups and i asked them if my drawing frightened them.',\n",
       "  'onset': 39.608,\n",
       "  'offset': 44.011,\n",
       "  'section': 1},\n",
       " {'sentence': 'they answered me why would anyone be frightened by a hat.',\n",
       "  'onset': 44.39,\n",
       "  'offset': 48.092,\n",
       "  'section': 1},\n",
       " {'sentence': 'my drawing was not of a hat.',\n",
       "  'onset': 49.13,\n",
       "  'offset': 50.645,\n",
       "  'section': 1},\n",
       " {'sentence': 'it showed a boa constrictor digesting an elephant.',\n",
       "  'onset': 51.109,\n",
       "  'offset': 54.232,\n",
       "  'section': 1}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract as lists, for each section individually\n",
    "data = []\n",
    "for section in possible_sections:\n",
    "    df_by_section = df[df['section'] == section]\n",
    "    words, onsets, offsets, sections = df_by_section['word'].tolist(), df_by_section['onset'].tolist(), df_by_section['offset'].tolist(), df_by_section['section'].tolist()\n",
    "\n",
    "    # create list of dicts\n",
    "    section_data = []\n",
    "    sentence = \"\"\n",
    "    temp_onsets, temp_offsets, temp_sections = [], [], []\n",
    "    for i, word in enumerate(words):\n",
    "        sentence = sentence + word + \" \"\n",
    "        temp_offsets.append(offsets[i])\n",
    "        temp_onsets.append(onsets[i])\n",
    "        temp_sections.append(sections[i])\n",
    "        \n",
    "        \n",
    "        if word[-1] == \"#\":\n",
    "            section_data.append({\"sentence\": sentence[:-2] + '.', \"onset\": temp_onsets[0], \"offset\": temp_offsets[-1], \"section\": sections[0]})\n",
    "            sentence, temp_onsets, temp_offsets, temp_sections = \"\", [], [], [] # reset\n",
    "    data.append(section_data)\n",
    "           \n",
    "data[0][:10]    # first 10 sentences of section 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the aligned text pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "PATH = \"/project/gpuuva021/shared/FMRI-Data\"\n",
    "OUTPATH = f\"{PATH}/aligned/\"\n",
    "SENT_N = 2  # chunksize: nr. of sentences (of the same section)\n",
    "lang = \"EN\"\n",
    "\n",
    "with open(\n",
    "        f\"{PATH}/text_data/{lang}_chunk_data_chunk_size_{SENT_N}.pickle\", \"rb\"\n",
    "    ) as f:\n",
    "        aligned_words = pickle.load(\n",
    "            f\n",
    "        )  # each section contains list of chunks which is a dict with keys sentence, onset, offset, section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,\n",
       " 65,\n",
       " dict_keys(['sentences', 'onset', 'offset', 'section']),\n",
       " 'once when i was six years old i saw a magnificent picture in a book about the primeval forest called real life stories. it showed a boa constrictor swallowing a wild animal. ')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aligned_words), len(aligned_words[0]), aligned_words[0][0].keys(), aligned_words[0][0]['sentences']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Get Sentences from Dependency File)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17201\n",
      "no nr found.\n",
      "conj:et(laisser-8, intéresser-22''')\n",
      "36\n",
      "conj:et(laisser-8, intéresser-22\n",
      "no nr found.\n",
      "conj:et(intéresser-22, intéresser-22''')\n",
      "40\n",
      "conj:et(intéresser-22, intéresser-22\n",
      "no nr found.\n",
      "nmod:de(bridge-5, golf-7''')\n",
      "28\n",
      "nmod:de(bridge-5, golf-7\n",
      "no nr found.\n",
      "conj:et(golf-7, golf-7''')\n",
      "26\n",
      "conj:et(golf-7, golf-7\n",
      "no nr found.\n",
      "conj:ou(question-6, question-6''')\n",
      "34\n",
      "conj:ou(question-6, question-6\n",
      "no nr found.\n",
      "advcl(hésite-2, tâtonne-12''')\n",
      "30\n",
      "advcl(hésite-2, tâtonne-12\n",
      "no nr found.\n",
      "conj:et(tâtonne-12, tâtonne-12''')\n",
      "34\n",
      "conj:et(tâtonne-12, tâtonne-12\n",
      "no nr found.\n",
      "obl:comme(avait-13, planètes-18''')\n",
      "35\n",
      "obl:comme(avait-13, planètes-18\n",
      "no nr found.\n",
      "conj:et(planètes-18, planètes-18''')\n",
      "36\n",
      "conj:et(planètes-18, planètes-18\n",
      "no nr found.\n",
      "obl:arg(agit-4, brindille-7''')\n",
      "31\n",
      "obl:arg(agit-4, brindille-7\n",
      "no nr found.\n",
      "conj:ou(brindille-7, brindille-7''')\n",
      "36\n",
      "conj:ou(brindille-7, brindille-7\n",
      "no nr found.\n",
      "nmod:de(marteau-12, boulon-15''')\n",
      "33\n",
      "nmod:de(marteau-12, boulon-15\n",
      "no nr found.\n",
      "conj:et(boulon-15, boulon-15''')\n",
      "32\n",
      "conj:et(boulon-15, boulon-15\n",
      "no nr found.\n",
      "xcomp(dû-3, juger-5''')\n",
      "23\n",
      "xcomp(dû-3, juger-5\n",
      "no nr found.\n",
      "conj:et(juger-5, juger-5''')\n",
      "28\n",
      "conj:et(juger-5, juger-5\n",
      "no nr found.\n",
      "xcomp(commença-2, visiter-6''')\n",
      "31\n",
      "xcomp(commença-2, visiter-6\n",
      "no nr found.\n",
      "conj:et(visiter-6, visiter-6''')\n",
      "32\n",
      "conj:et(visiter-6, visiter-6\n",
      "no nr found.\n",
      "obl:à(ordonnais-9, général-12''')\n",
      "33\n",
      "obl:à(ordonnais-9, général-12\n",
      "no nr found.\n",
      "conj:et(général-12, général-12''')\n",
      "34\n",
      "conj:et(général-12, général-12\n",
      "no nr found.\n",
      "xcomp(pu-10, assister-11''''''''''''''''''')\n",
      "44\n",
      "xcomp(pu-10, assister-11''''''''''''''''\n",
      "no nr found.\n",
      "conj:ou(assister-11, assister-11''''''''''''''''''')\n",
      "52\n",
      "conj:ou(assister-11, assister-11''''''''''''''''\n",
      "no nr found.\n",
      "xcomp(pu-10, assister-11'''''''''''''''''''')\n",
      "45\n",
      "xcomp(pu-10, assister-11'''''''''''''''''\n",
      "no nr found.\n",
      "conj:ou(assister-11, assister-11'''''''''''''''''''')\n",
      "53\n",
      "conj:ou(assister-11, assister-11'''''''''''''''''\n",
      "no nr found.\n",
      "xcomp(pu-10, assister-11''''''''''''''''''''')\n",
      "46\n",
      "xcomp(pu-10, assister-11''''''''''''''''''\n",
      "no nr found.\n",
      "conj:ou(assister-11, assister-11''''''''''''''''''''')\n",
      "54\n",
      "conj:ou(assister-11, assister-11''''''''''''''''''\n",
      "no nr found.\n",
      "xcomp(pu-10, assister-11'''''''''''''''''''''')\n",
      "47\n",
      "xcomp(pu-10, assister-11'''''''''''''''''''\n",
      "no nr found.\n",
      "conj:ou(assister-11, assister-11'''''''''''''''''''''')\n",
      "55\n",
      "conj:ou(assister-11, assister-11'''''''''''''''''''\n",
      "no nr found.\n",
      "xcomp(pu-10, assister-11''''''''''''''''''''''')\n",
      "48\n",
      "xcomp(pu-10, assister-11''''''''''''''''''''\n",
      "no nr found.\n",
      "conj:ou(assister-11, assister-11''''''''''''''''''''''')\n",
      "56\n",
      "conj:ou(assister-11, assister-11''''''''''''''''''''\n",
      "no nr found.\n",
      "xcomp(pu-10, assister-11'''''''''''''''''''''''')\n",
      "49\n",
      "xcomp(pu-10, assister-11'''''''''''''''''''''\n",
      "no nr found.\n",
      "conj:ou(assister-11, assister-11'''''''''''''''''''''''')\n",
      "57\n",
      "conj:ou(assister-11, assister-11'''''''''''''''''''''\n",
      "no nr found.\n",
      "xcomp(pu-10, assister-11''''''''''''''''''''''''')\n",
      "50\n",
      "xcomp(pu-10, assister-11''''''''''''''''''''''\n",
      "no nr found.\n",
      "conj:mais(assister-11, assister-11''''''''''''''''''''''''')\n",
      "60\n",
      "conj:mais(assister-11, assister-11''''''''''''''''''''''\n",
      "no nr found.\n",
      "xcomp(pu-10, assister-11'''''''''''''''''''''''''')\n",
      "51\n",
      "xcomp(pu-10, assister-11'''''''''''''''''''''''\n",
      "no nr found.\n",
      "conj:mais(assister-11, assister-11'''''''''''''''''''''''''')\n",
      "61\n",
      "conj:mais(assister-11, assister-11'''''''''''''''''''''''\n",
      "no nr found.\n",
      "xcomp(pu-10, assister-11''''''''''''''''''''''''''')\n",
      "52\n",
      "xcomp(pu-10, assister-11''''''''''''''''''''''''\n",
      "no nr found.\n",
      "conj:mais(assister-11, assister-11''''''''''''''''''''''''''')\n",
      "62\n",
      "conj:mais(assister-11, assister-11''''''''''''''''''''''''\n",
      "no nr found.\n",
      "obl:à(ordonnais-3, général-6''')\n",
      "32\n",
      "obl:à(ordonnais-3, général-6\n",
      "no nr found.\n",
      "conj:et(général-6, général-6''')\n",
      "32\n",
      "conj:et(général-6, général-6\n",
      "no nr found.\n",
      "obl:à(voler-8, façon-17''''''''')\n",
      "33\n",
      "obl:à(voler-8, façon-17''''''\n",
      "no nr found.\n",
      "conj:ou(façon-17, façon-17''''''''')\n",
      "36\n",
      "conj:ou(façon-17, façon-17''''''\n",
      "no nr found.\n",
      "obl:à(voler-8, façon-17'''''''''')\n",
      "34\n",
      "obl:à(voler-8, façon-17'''''''\n",
      "no nr found.\n",
      "conj:ou(façon-17, façon-17'''''''''')\n",
      "37\n",
      "conj:ou(façon-17, façon-17'''''''\n",
      "no nr found.\n",
      "obl:à(voler-8, façon-17''''''''''')\n",
      "35\n",
      "obl:à(voler-8, façon-17''''''''\n",
      "no nr found.\n",
      "conj:ou(façon-17, façon-17''''''''''')\n",
      "38\n",
      "conj:ou(façon-17, façon-17''''''''\n",
      "no nr found.\n",
      "obl:à(voler-8, façon-17'''''''''''')\n",
      "36\n",
      "obl:à(voler-8, façon-17'''''''''\n",
      "no nr found.\n",
      "conj:ou(façon-17, façon-17'''''''''''')\n",
      "39\n",
      "conj:ou(façon-17, façon-17'''''''''\n",
      "no nr found.\n",
      "conj:ou(qui-44, qui-44''')\n",
      "26\n",
      "conj:ou(qui-44, qui-44\n",
      "no nr found.\n",
      "nsubj(tort-53, qui-44''')\n",
      "25\n",
      "nsubj(tort-53, qui-44\n",
      "no nr found.\n",
      "nmod:de(tour-5, allumeurs-8''')\n",
      "31\n",
      "nmod:de(tour-5, allumeurs-8\n",
      "no nr found.\n",
      "conj:et(allumeurs-8, allumeurs-8''')\n",
      "36\n",
      "conj:et(allumeurs-8, allumeurs-8\n",
      "no nr found.\n",
      "obj(entraient-2, allumeurs-10''')\n",
      "33\n",
      "obj(entraient-2, allumeurs-10\n",
      "no nr found.\n",
      "conj:et(allumeurs-10, allumeurs-10''')\n",
      "38\n",
      "conj:et(allumeurs-10, allumeurs-10\n",
      "no nr found.\n",
      "nmod:de(tour-4, allumeurs-7''')\n",
      "31\n",
      "nmod:de(tour-4, allumeurs-7\n",
      "no nr found.\n",
      "conj:et(allumeurs-7, allumeurs-7''')\n",
      "36\n",
      "conj:et(allumeurs-7, allumeurs-7\n",
      "no nr found.\n",
      "conj:et(ceux-3, ceux-3''')\n",
      "26\n",
      "conj:et(ceux-3, ceux-3\n",
      "no nr found.\n",
      "obj(menaient-23, vies-25''')\n",
      "28\n",
      "obj(menaient-23, vies-25\n",
      "no nr found.\n",
      "conj:et(vies-25, vies-25''')\n",
      "28\n",
      "conj:et(vies-25, vies-25\n",
      "no nr found.\n",
      "obj(rêve-12, histoire-14''')\n",
      "28\n",
      "obj(rêve-12, histoire-14\n",
      "no nr found.\n",
      "conj:et(histoire-14, histoire-14''')\n",
      "36\n",
      "conj:et(histoire-14, histoire-14\n",
      "no nr found.\n",
      "xcomp(donnée-6, musique-12''')\n",
      "30\n",
      "xcomp(donnée-6, musique-12\n",
      "no nr found.\n",
      "conj:et(musique-12, musique-12''')\n",
      "34\n",
      "conj:et(musique-12, musique-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1366"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "def sentences_from_dep_file(PATH):\n",
    "    with open(PATH, \"r\", encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    sentences = ['']\n",
    "    \n",
    "    print(len(lines))\n",
    "    \n",
    "    for line in lines:\n",
    "        if line != '\\n':\n",
    "            line = line.strip()\n",
    "            # print(line)\n",
    "            \n",
    "            word = re.search(r\",\\s*(.*?)\\s*-\\d+\", line)\n",
    "            nr = re.search(r\"-(\\d+\\))\", line)\n",
    "            \n",
    "            if word:\n",
    "                result_word = word.group(1).strip()\n",
    "                result_word = result_word + \" \"\n",
    "            else:\n",
    "                print(\"No word found.\")\n",
    "                print(line)\n",
    "                continue\n",
    "                \n",
    "            if nr:\n",
    "                result_nr = nr.group(1).strip()\n",
    "            else:\n",
    "                \n",
    "                if line[:-3] == f\"''')\":\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"no nr found.\")\n",
    "                    print(line)\n",
    "                    print(len(line))\n",
    "                    print(line[:-4])\n",
    "                    continue\n",
    "        \n",
    "            # print(result_word, result_nr)\n",
    "            \n",
    "            \n",
    "            if result_nr == '1)':\n",
    "                # print(f'FIrst sent first word: {result_word}')\n",
    "                sentences.append(result_word)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                # print(f'{result_word}')\n",
    "                sentences[-1] = sentences[-1] + result_word\n",
    "              \n",
    "    return sentences\n",
    "\n",
    "sents=sentences_from_dep_file('/project/gpuuva021/shared/FMRI-Data/annotation/FR/lppFR_dependency.csv')\n",
    "sents\n",
    "len(sents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
