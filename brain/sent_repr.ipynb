{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get BERT Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepare input\n",
    "# text = \"Replace me by any text you'd like BERT to be fine-tuned on\"\n",
    "# encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# # forward pass\n",
    "# output = model(**encoded_input, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_hidden_state = output.hidden_states[-1] # shape: batch_size, sequence_length, hidden_size\n",
    "# h_s = output.hidden_states # tuple of length 13\n",
    "\n",
    "# # Extract the CLS embedding\n",
    "# cls_embedding = last_hidden_state[:, 0, :]\n",
    "\n",
    "# last_hidden_state.shape, cls_embedding.shape, len(h_s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Sentences from Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "import pandas as pd\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 0\n",
      "Errors: 0\n",
      "Errors: 0\n"
     ]
    }
   ],
   "source": [
    "def extract_words_from_tree(tree):\n",
    "    words = []\n",
    "    if isinstance(tree, str):  # Base case: leaf node (word)\n",
    "        return [tree]\n",
    "    \n",
    "    elif isinstance(tree, Tree):\n",
    "        for subtree in tree:\n",
    "            words.extend(extract_words_from_tree(subtree))\n",
    "            \n",
    "    #sentence = \" \".join(words)\n",
    "    return words\n",
    "\n",
    "\n",
    "def extract_sent_list_from_tree_file(PATH):\n",
    "    \n",
    "    with open(PATH, \"r\", encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    sentences = []\n",
    "    counter = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        try:\n",
    "            tree = Tree.fromstring(line)\n",
    "        except ValueError:\n",
    "            try: # remove last ')'\n",
    "                tree = Tree.fromstring(line[:-1])\n",
    "                \n",
    "            except ValueError:\n",
    "                counter += 1\n",
    "                print(f\"=== ValueError: line {i} \\n {line} ===\")\n",
    "                continue\n",
    "        words = extract_words_from_tree(tree)\n",
    "        sentences.append(words)\n",
    "        \n",
    "    print(f'Errors: {counter}')\n",
    "    return sentences\n",
    "    \n",
    "sentences_EN = extract_sent_list_from_tree_file('/project/gpuuva021/shared/FMRI-Data/annotation/EN/lppEN_tree.csv')\n",
    "sentences_CN = extract_sent_list_from_tree_file('/project/gpuuva021/shared/FMRI-Data/annotation/CN/lppCN_tree.csv')\n",
    "sentences_FR = extract_sent_list_from_tree_file('/project/gpuuva021/shared/FMRI-Data/annotation/FR/lppFR_tree.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['je', 'ne', 'le', 'savais', 'pas']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TREE TEST\n",
    "line =  '(S (VN (CLS je) (ADV ne) (CLO le) (V savais)) (ADV pas))'\n",
    "tree = Tree.fromstring(line)\n",
    "words = extract_words_from_tree(tree)\n",
    "words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Sentence infos from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"/project/gpuuva021/shared/FMRI-Data\")\n",
    "language = \"EN\"\n",
    "\n",
    "# /project/gpuuva021/shared/FMRI-Data/annotation/EN/lppEN_word_information.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "      <th>logfreq</th>\n",
       "      <th>pos</th>\n",
       "      <th>section</th>\n",
       "      <th>top_down</th>\n",
       "      <th>bottom_up</th>\n",
       "      <th>left_corner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>once</td>\n",
       "      <td>once</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.728</td>\n",
       "      <td>5.824406</td>\n",
       "      <td>ADV</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.919</td>\n",
       "      <td>7.562214</td>\n",
       "      <td>ADV</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "      <td>0.919</td>\n",
       "      <td>1.025</td>\n",
       "      <td>8.500759</td>\n",
       "      <td>PRON</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>was</td>\n",
       "      <td>was</td>\n",
       "      <td>1.025</td>\n",
       "      <td>1.158</td>\n",
       "      <td>8.848911</td>\n",
       "      <td>AUX</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>six</td>\n",
       "      <td>six</td>\n",
       "      <td>1.158</td>\n",
       "      <td>1.464</td>\n",
       "      <td>5.208940</td>\n",
       "      <td>NUM</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  word lemma  onset  offset   logfreq   pos  section  top_down   \n",
       "0           0  once  once  0.113   0.728  5.824406   ADV        1         3  \\\n",
       "1           1  when  when  0.728   0.919  7.562214   ADV        1         2   \n",
       "2           2     i     i  0.919   1.025  8.500759  PRON        1         3   \n",
       "3           3   was   was  1.025   1.158  8.848911   AUX        1         2   \n",
       "4           4   six   six  1.158   1.464  5.208940   NUM        1         3   \n",
       "\n",
       "   bottom_up  left_corner  \n",
       "0          1            2  \n",
       "1          2            2  \n",
       "2          2            3  \n",
       "3          1            2  \n",
       "4          1            2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "            PATH / f\"annotation/{language}/lpp{language}_word_information.csv\"\n",
    "        )\n",
    "\n",
    "df_list = df['word'].tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['once',\n",
       " 'when',\n",
       " 'i',\n",
       " 'was',\n",
       " 'six',\n",
       " 'years',\n",
       " 'old',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'a',\n",
       " 'magnificent',\n",
       " 'picture',\n",
       " 'in',\n",
       " 'a',\n",
       " 'book',\n",
       " 'about',\n",
       " 'the',\n",
       " 'primeval',\n",
       " 'forest',\n",
       " 'called',\n",
       " 'real',\n",
       " 'life',\n",
       " 'stories']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_EN[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not',\n",
       " 'respond',\n",
       " 'when',\n",
       " 'questioned',\n",
       " 'you',\n",
       " 'will',\n",
       " 'easily',\n",
       " 'guess',\n",
       " 'who',\n",
       " 'it',\n",
       " 'is#',\n",
       " 'so',\n",
       " 'think',\n",
       " 'of',\n",
       " 'me#',\n",
       " 'save',\n",
       " 'me',\n",
       " 'from',\n",
       " 'this',\n",
       " 'sorrow#',\n",
       " 'write',\n",
       " 'to',\n",
       " 'me',\n",
       " 'quickly',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'me',\n",
       " 'he',\n",
       " 'is',\n",
       " 'back#']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after the last word of each sentence, add # to indicate the end of sentence\n",
    "for i, sent in enumerate(sentences_EN):\n",
    "    sentences_EN[i][-1] = sent[-1] + \"#\" # replace the last word with the word + #\n",
    "\n",
    "# flatten the list of lists of words into a list of words\n",
    "words = [item for sublist in sentences_EN for item in sublist]\n",
    "\n",
    "words[-30:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15376, 15376)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words), len(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>once</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.728</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.919</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i</td>\n",
       "      <td>0.919</td>\n",
       "      <td>1.025</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>was</td>\n",
       "      <td>1.025</td>\n",
       "      <td>1.158</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>six</td>\n",
       "      <td>1.158</td>\n",
       "      <td>1.464</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word  onset  offset  section\n",
       "0  once  0.113   0.728        1\n",
       "1  when  0.728   0.919        1\n",
       "2     i  0.919   1.025        1\n",
       "3   was  1.025   1.158        1\n",
       "4   six  1.158   1.464        1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# integrate back into df\n",
    "df['word'] = words\n",
    "\n",
    "# keep only relevant columns\n",
    "df = df[['word', 'onset', 'offset', 'section']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'once when i was six years old i saw a magnificent picture in a book about the primeval forest called real life stories.',\n",
       "  'onset': 0.113,\n",
       "  'offset': 8.339,\n",
       "  'section': 1},\n",
       " {'sentence': 'it showed a boa constrictor swallowing a wild animal.',\n",
       "  'onset': 9.247,\n",
       "  'offset': 12.416,\n",
       "  'section': 1}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract as lists\n",
    "onsets, offsets, sections = df['onset'].tolist(), df['offset'].tolist(), df['section'].tolist()\n",
    "\n",
    "# create list of dicts\n",
    "data = []\n",
    "sentence = \"\"\n",
    "temp_onsets, temp_offsets, temp_sections = [], [], []\n",
    "for i, word in enumerate(words):\n",
    "    sentence = sentence + word + \" \"\n",
    "    temp_offsets.append(offsets[i])\n",
    "    temp_onsets.append(onsets[i])\n",
    "    temp_sections.append(sections[i])\n",
    "    \n",
    "    \n",
    "    if word[-1] == \"#\":\n",
    "        data.append({\"sentence\": sentence[:-2] + '.', \"onset\": temp_onsets[0], \"offset\": temp_offsets[-1], \"section\": sections[0]})\n",
    "        sentence, temp_onsets, temp_offsets, temp_sections = \"\", [], [], [] # reset\n",
    "        \n",
    "data[:2]      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Get Sentences from Dependency File)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17201\n",
      "no nr found.\n",
      "conj:et(laisser-8, intéresser-22''')\n",
      "36\n",
      "conj:et(laisser-8, intéresser-22\n",
      "no nr found.\n",
      "conj:et(intéresser-22, intéresser-22''')\n",
      "40\n",
      "conj:et(intéresser-22, intéresser-22\n",
      "no nr found.\n",
      "nmod:de(bridge-5, golf-7''')\n",
      "28\n",
      "nmod:de(bridge-5, golf-7\n",
      "no nr found.\n",
      "conj:et(golf-7, golf-7''')\n",
      "26\n",
      "conj:et(golf-7, golf-7\n",
      "no nr found.\n",
      "conj:ou(question-6, question-6''')\n",
      "34\n",
      "conj:ou(question-6, question-6\n",
      "no nr found.\n",
      "advcl(hésite-2, tâtonne-12''')\n",
      "30\n",
      "advcl(hésite-2, tâtonne-12\n",
      "no nr found.\n",
      "conj:et(tâtonne-12, tâtonne-12''')\n",
      "34\n",
      "conj:et(tâtonne-12, tâtonne-12\n",
      "no nr found.\n",
      "obl:comme(avait-13, planètes-18''')\n",
      "35\n",
      "obl:comme(avait-13, planètes-18\n",
      "no nr found.\n",
      "conj:et(planètes-18, planètes-18''')\n",
      "36\n",
      "conj:et(planètes-18, planètes-18\n",
      "no nr found.\n",
      "obl:arg(agit-4, brindille-7''')\n",
      "31\n",
      "obl:arg(agit-4, brindille-7\n",
      "no nr found.\n",
      "conj:ou(brindille-7, brindille-7''')\n",
      "36\n",
      "conj:ou(brindille-7, brindille-7\n",
      "no nr found.\n",
      "nmod:de(marteau-12, boulon-15''')\n",
      "33\n",
      "nmod:de(marteau-12, boulon-15\n",
      "no nr found.\n",
      "conj:et(boulon-15, boulon-15''')\n",
      "32\n",
      "conj:et(boulon-15, boulon-15\n",
      "no nr found.\n",
      "xcomp(dû-3, juger-5''')\n",
      "23\n",
      "xcomp(dû-3, juger-5\n",
      "no nr found.\n",
      "conj:et(juger-5, juger-5''')\n",
      "28\n",
      "conj:et(juger-5, juger-5\n",
      "no nr found.\n",
      "xcomp(commença-2, visiter-6''')\n",
      "31\n",
      "xcomp(commença-2, visiter-6\n",
      "no nr found.\n",
      "conj:et(visiter-6, visiter-6''')\n",
      "32\n",
      "conj:et(visiter-6, visiter-6\n",
      "no nr found.\n",
      "obl:à(ordonnais-9, général-12''')\n",
      "33\n",
      "obl:à(ordonnais-9, général-12\n",
      "no nr found.\n",
      "conj:et(général-12, général-12''')\n",
      "34\n",
      "conj:et(général-12, général-12\n",
      "no nr found.\n",
      "xcomp(pu-10, assister-11''''''''''''''''''')\n",
      "44\n",
      "xcomp(pu-10, assister-11''''''''''''''''\n",
      "no nr found.\n",
      "conj:ou(assister-11, assister-11''''''''''''''''''')\n",
      "52\n",
      "conj:ou(assister-11, assister-11''''''''''''''''\n",
      "no nr found.\n",
      "xcomp(pu-10, assister-11'''''''''''''''''''')\n",
      "45\n",
      "xcomp(pu-10, assister-11'''''''''''''''''\n",
      "no nr found.\n",
      "conj:ou(assister-11, assister-11'''''''''''''''''''')\n",
      "53\n",
      "conj:ou(assister-11, assister-11'''''''''''''''''\n",
      "no nr found.\n",
      "xcomp(pu-10, assister-11''''''''''''''''''''')\n",
      "46\n",
      "xcomp(pu-10, assister-11''''''''''''''''''\n",
      "no nr found.\n",
      "conj:ou(assister-11, assister-11''''''''''''''''''''')\n",
      "54\n",
      "conj:ou(assister-11, assister-11''''''''''''''''''\n",
      "no nr found.\n",
      "xcomp(pu-10, assister-11'''''''''''''''''''''')\n",
      "47\n",
      "xcomp(pu-10, assister-11'''''''''''''''''''\n",
      "no nr found.\n",
      "conj:ou(assister-11, assister-11'''''''''''''''''''''')\n",
      "55\n",
      "conj:ou(assister-11, assister-11'''''''''''''''''''\n",
      "no nr found.\n",
      "xcomp(pu-10, assister-11''''''''''''''''''''''')\n",
      "48\n",
      "xcomp(pu-10, assister-11''''''''''''''''''''\n",
      "no nr found.\n",
      "conj:ou(assister-11, assister-11''''''''''''''''''''''')\n",
      "56\n",
      "conj:ou(assister-11, assister-11''''''''''''''''''''\n",
      "no nr found.\n",
      "xcomp(pu-10, assister-11'''''''''''''''''''''''')\n",
      "49\n",
      "xcomp(pu-10, assister-11'''''''''''''''''''''\n",
      "no nr found.\n",
      "conj:ou(assister-11, assister-11'''''''''''''''''''''''')\n",
      "57\n",
      "conj:ou(assister-11, assister-11'''''''''''''''''''''\n",
      "no nr found.\n",
      "xcomp(pu-10, assister-11''''''''''''''''''''''''')\n",
      "50\n",
      "xcomp(pu-10, assister-11''''''''''''''''''''''\n",
      "no nr found.\n",
      "conj:mais(assister-11, assister-11''''''''''''''''''''''''')\n",
      "60\n",
      "conj:mais(assister-11, assister-11''''''''''''''''''''''\n",
      "no nr found.\n",
      "xcomp(pu-10, assister-11'''''''''''''''''''''''''')\n",
      "51\n",
      "xcomp(pu-10, assister-11'''''''''''''''''''''''\n",
      "no nr found.\n",
      "conj:mais(assister-11, assister-11'''''''''''''''''''''''''')\n",
      "61\n",
      "conj:mais(assister-11, assister-11'''''''''''''''''''''''\n",
      "no nr found.\n",
      "xcomp(pu-10, assister-11''''''''''''''''''''''''''')\n",
      "52\n",
      "xcomp(pu-10, assister-11''''''''''''''''''''''''\n",
      "no nr found.\n",
      "conj:mais(assister-11, assister-11''''''''''''''''''''''''''')\n",
      "62\n",
      "conj:mais(assister-11, assister-11''''''''''''''''''''''''\n",
      "no nr found.\n",
      "obl:à(ordonnais-3, général-6''')\n",
      "32\n",
      "obl:à(ordonnais-3, général-6\n",
      "no nr found.\n",
      "conj:et(général-6, général-6''')\n",
      "32\n",
      "conj:et(général-6, général-6\n",
      "no nr found.\n",
      "obl:à(voler-8, façon-17''''''''')\n",
      "33\n",
      "obl:à(voler-8, façon-17''''''\n",
      "no nr found.\n",
      "conj:ou(façon-17, façon-17''''''''')\n",
      "36\n",
      "conj:ou(façon-17, façon-17''''''\n",
      "no nr found.\n",
      "obl:à(voler-8, façon-17'''''''''')\n",
      "34\n",
      "obl:à(voler-8, façon-17'''''''\n",
      "no nr found.\n",
      "conj:ou(façon-17, façon-17'''''''''')\n",
      "37\n",
      "conj:ou(façon-17, façon-17'''''''\n",
      "no nr found.\n",
      "obl:à(voler-8, façon-17''''''''''')\n",
      "35\n",
      "obl:à(voler-8, façon-17''''''''\n",
      "no nr found.\n",
      "conj:ou(façon-17, façon-17''''''''''')\n",
      "38\n",
      "conj:ou(façon-17, façon-17''''''''\n",
      "no nr found.\n",
      "obl:à(voler-8, façon-17'''''''''''')\n",
      "36\n",
      "obl:à(voler-8, façon-17'''''''''\n",
      "no nr found.\n",
      "conj:ou(façon-17, façon-17'''''''''''')\n",
      "39\n",
      "conj:ou(façon-17, façon-17'''''''''\n",
      "no nr found.\n",
      "conj:ou(qui-44, qui-44''')\n",
      "26\n",
      "conj:ou(qui-44, qui-44\n",
      "no nr found.\n",
      "nsubj(tort-53, qui-44''')\n",
      "25\n",
      "nsubj(tort-53, qui-44\n",
      "no nr found.\n",
      "nmod:de(tour-5, allumeurs-8''')\n",
      "31\n",
      "nmod:de(tour-5, allumeurs-8\n",
      "no nr found.\n",
      "conj:et(allumeurs-8, allumeurs-8''')\n",
      "36\n",
      "conj:et(allumeurs-8, allumeurs-8\n",
      "no nr found.\n",
      "obj(entraient-2, allumeurs-10''')\n",
      "33\n",
      "obj(entraient-2, allumeurs-10\n",
      "no nr found.\n",
      "conj:et(allumeurs-10, allumeurs-10''')\n",
      "38\n",
      "conj:et(allumeurs-10, allumeurs-10\n",
      "no nr found.\n",
      "nmod:de(tour-4, allumeurs-7''')\n",
      "31\n",
      "nmod:de(tour-4, allumeurs-7\n",
      "no nr found.\n",
      "conj:et(allumeurs-7, allumeurs-7''')\n",
      "36\n",
      "conj:et(allumeurs-7, allumeurs-7\n",
      "no nr found.\n",
      "conj:et(ceux-3, ceux-3''')\n",
      "26\n",
      "conj:et(ceux-3, ceux-3\n",
      "no nr found.\n",
      "obj(menaient-23, vies-25''')\n",
      "28\n",
      "obj(menaient-23, vies-25\n",
      "no nr found.\n",
      "conj:et(vies-25, vies-25''')\n",
      "28\n",
      "conj:et(vies-25, vies-25\n",
      "no nr found.\n",
      "obj(rêve-12, histoire-14''')\n",
      "28\n",
      "obj(rêve-12, histoire-14\n",
      "no nr found.\n",
      "conj:et(histoire-14, histoire-14''')\n",
      "36\n",
      "conj:et(histoire-14, histoire-14\n",
      "no nr found.\n",
      "xcomp(donnée-6, musique-12''')\n",
      "30\n",
      "xcomp(donnée-6, musique-12\n",
      "no nr found.\n",
      "conj:et(musique-12, musique-12''')\n",
      "34\n",
      "conj:et(musique-12, musique-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1366"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "def sentences_from_dep_file(PATH):\n",
    "    with open(PATH, \"r\", encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    sentences = ['']\n",
    "    \n",
    "    print(len(lines))\n",
    "    \n",
    "    for line in lines:\n",
    "        if line != '\\n':\n",
    "            line = line.strip()\n",
    "            # print(line)\n",
    "            \n",
    "            word = re.search(r\",\\s*(.*?)\\s*-\\d+\", line)\n",
    "            nr = re.search(r\"-(\\d+\\))\", line)\n",
    "            \n",
    "            if word:\n",
    "                result_word = word.group(1).strip()\n",
    "                result_word = result_word + \" \"\n",
    "            else:\n",
    "                print(\"No word found.\")\n",
    "                print(line)\n",
    "                continue\n",
    "                \n",
    "            if nr:\n",
    "                result_nr = nr.group(1).strip()\n",
    "            else:\n",
    "                \n",
    "                if line[:-3] == f\"''')\":\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"no nr found.\")\n",
    "                    print(line)\n",
    "                    print(len(line))\n",
    "                    print(line[:-4])\n",
    "                    continue\n",
    "        \n",
    "            # print(result_word, result_nr)\n",
    "            \n",
    "            \n",
    "            if result_nr == '1)':\n",
    "                # print(f'FIrst sent first word: {result_word}')\n",
    "                sentences.append(result_word)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                # print(f'{result_word}')\n",
    "                sentences[-1] = sentences[-1] + result_word\n",
    "              \n",
    "    return sentences\n",
    "\n",
    "sents=sentences_from_dep_file('/project/gpuuva021/shared/FMRI-Data/annotation/FR/lppFR_dependency.csv')\n",
    "sents\n",
    "len(sents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
